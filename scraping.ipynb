{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grcJ9rh0iNe8"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Authenticating Twitter API\n",
    "# Obtain your Twitter credentials from your twitter developer account\n",
    "\n",
    "consumer_key = 'your_consumer_key'\n",
    "consumer_secret = 'your_consumer_secret'\n",
    "access_key = 'your_access_key'\n",
    "access_secret = 'your_access_secret'\n",
    "\n",
    "# Pass your twitter credentials to tweepy via its OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "## Automating Scraping\n",
    "# Calls API every 15 minutes to prevent overcalling\n",
    "\n",
    "# 1. define a for-loop\n",
    "# 2. define search parameter\n",
    "# 3. define date period\n",
    "# 4. define no. of tweets to pull\n",
    "\n",
    "def scraptweets(search_words, date_since, date_until, numTweets, numRuns):\n",
    "\n",
    "    ## Arguments:\n",
    "    # search_words -> define a string of keywords for this function to extract\n",
    "    # date_since -> define a date from which to start extracting the tweets \n",
    "    # date_until -> define the date to which to stop extracting the tweets\n",
    "    # numTweets -> number of tweets to extract per run\n",
    "    # numRun -> number of runs to perform in this program - API calls are limited to once every 15 mins, so each run will be 15 mins apart.\n",
    "    ##\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'text', 'hashtags']\n",
    "                                )\n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, until=date_until, tweet_mode='extended').items(numTweets)\n",
    "\n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "\n",
    "        # Obtain the following info (methods to call them out):\n",
    "            # user.screen_name - twitter handle\n",
    "            # user.description - description of account\n",
    "            # user.location - where is he tweeting from\n",
    "            # user.friends_count - no. of other users that user is following (following)\n",
    "            # user.followers_count - no. of other users who are following this user (followers)\n",
    "            # user.statuses_count - total tweets by user\n",
    "            # user.created_at - when the user account was created\n",
    "            # created_at - when the tweet was created\n",
    "            # retweet_count - no. of retweets\n",
    "            # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
    "            # retweeted_status.full_text - full text of the tweet\n",
    "            # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "\n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "\n",
    "        for tweet in tweet_list:\n",
    "\n",
    "            # Pull the values\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "\n",
    "            try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                text = tweet.full_text\n",
    "\n",
    "            # Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
    "\n",
    "            # Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "            # increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "        \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round(end_run-start_run, 2)\n",
    "        \n",
    "        print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
    "        print('time take for {} run to complete is {}'.format(i, duration_run))\n",
    "        \n",
    "        time.sleep(900) #15 minute sleep time\n",
    "\n",
    "        \n",
    "    # Once all runs have completed, save them to a single csv file:    \n",
    "    # Obtain timestamp in a readable format:\n",
    "    from datetime import datetime\n",
    "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_moi_tweets.csv'\n",
    "\n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    print('Scraping has completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the variables\n",
    "search_word = \"moi OR Moi OR #Moi OR ripmoi OR #MoiBodyViewing OR #Moidead OR #MoiFarewell OR #MoiFuneral\"\n",
    "date_since = \"2020-02-04\"\n",
    "date_until = \"2020-02-12\"\n",
    "numTweets = 2500\n",
    "numRuns = 6\n",
    "# Calling the fuction\n",
    "scraptweets(search_words, date_since, date_until, numTweets, numRuns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "twitter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
